{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb00322",
   "metadata": {},
   "source": [
    "# Segunda seção de modelos\n",
    "\n",
    "Variáveis criadas e originais com IV até 0.5 e que não são correlacionadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1fcd8d",
   "metadata": {},
   "source": [
    "# Pacotes e funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9181e2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score, average_precision_score, brier_score_loss, confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_curve, log_loss\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import xgboost\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm\n",
    "from hyperopt import fmin, tpe, Trials, hp\n",
    "from functools import partial\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e14be",
   "metadata": {},
   "source": [
    "# Bases pré-processadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4115f446",
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_full = pd.read_parquet('../3.Pre_processamento/treino_pp.parquet')\n",
    "val_full = pd.read_parquet('../3.Pre_processamento/validacao_pp.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cbadbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase amostral: 10% da base de treino\n",
    "treino = treino_full.sample(frac=.1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2cbd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segunda seção de modelos\n",
    "# Variáveis criadas e originais com IV até 0.5 e que não são correlacionadas\n",
    "colunas = ['used_pin_number', 'log_DFH_padro', 'log_DFLT_padro', 'RROO', 'used_chip']\n",
    "\n",
    "X_treino = treino[colunas]\n",
    "y_treino = treino['fraud']\n",
    "\n",
    "X_val = val_full[colunas]\n",
    "y_val = val_full['fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fbfb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fraud\n",
       "0    0.91424\n",
       "1    0.08576\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_treino.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a200bb",
   "metadata": {},
   "source": [
    "# Set do experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42da26eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'189457669412080989'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = mlflow.set_experiment(experiment_id = '189457669412080989')\n",
    "experiment.experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8757da",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb96f5f",
   "metadata": {},
   "source": [
    "## Regressão logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd2e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = sm.GLM(y_treino, sm.add_constant(X_treino), family=sm.families.Binomial())\n",
    "LR_results = LR.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf25e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>fraud</td>      <th>  No. Observations:  </th>  <td> 78300</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 78294</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -20496.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 29 Apr 2025</td> <th>  Deviance:          </th> <td>  40991.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>19:40:28</td>     <th>  Pearson chi2:      </th> <td>9.30e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>9</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.05985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>   -3.1605</td> <td>    0.033</td> <td>  -97.073</td> <td> 0.000</td> <td>   -3.224</td> <td>   -3.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>used_pin_number</th> <td>   -3.6261</td> <td>    0.209</td> <td>  -17.315</td> <td> 0.000</td> <td>   -4.037</td> <td>   -3.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_DFH_padro</th>   <td>    0.3313</td> <td>    0.015</td> <td>   22.826</td> <td> 0.000</td> <td>    0.303</td> <td>    0.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_DFLT_padro</th>  <td>    0.2085</td> <td>    0.013</td> <td>   15.814</td> <td> 0.000</td> <td>    0.183</td> <td>    0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RROO</th>            <td>    1.3823</td> <td>    0.035</td> <td>   39.484</td> <td> 0.000</td> <td>    1.314</td> <td>    1.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>used_chip</th>       <td>   -0.4942</td> <td>    0.030</td> <td>  -16.702</td> <td> 0.000</td> <td>   -0.552</td> <td>   -0.436</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      fraud       & \\textbf{  No. Observations:  } &    78300    \\\\\n",
       "\\textbf{Model:}            &       GLM        & \\textbf{  Df Residuals:      } &    78294    \\\\\n",
       "\\textbf{Model Family:}     &     Binomial     & \\textbf{  Df Model:          } &        5    \\\\\n",
       "\\textbf{Link Function:}    &      Logit       & \\textbf{  Scale:             } &    1.0000   \\\\\n",
       "\\textbf{Method:}           &       IRLS       & \\textbf{  Log-Likelihood:    } &   -20496.   \\\\\n",
       "\\textbf{Date:}             & Tue, 29 Apr 2025 & \\textbf{  Deviance:          } &    40991.   \\\\\n",
       "\\textbf{Time:}             &     19:40:28     & \\textbf{  Pearson chi2:      } &  9.30e+04   \\\\\n",
       "\\textbf{No. Iterations:}   &        9         & \\textbf{  Pseudo R-squ. (CS):} &  0.05985    \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                           & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}             &      -3.1605  &        0.033     &   -97.073  &         0.000        &       -3.224    &       -3.097     \\\\\n",
       "\\textbf{used\\_pin\\_number} &      -3.6261  &        0.209     &   -17.315  &         0.000        &       -4.037    &       -3.216     \\\\\n",
       "\\textbf{log\\_DFH\\_padro}   &       0.3313  &        0.015     &    22.826  &         0.000        &        0.303    &        0.360     \\\\\n",
       "\\textbf{log\\_DFLT\\_padro}  &       0.2085  &        0.013     &    15.814  &         0.000        &        0.183    &        0.234     \\\\\n",
       "\\textbf{RROO}              &       1.3823  &        0.035     &    39.484  &         0.000        &        1.314    &        1.451     \\\\\n",
       "\\textbf{used\\_chip}        &      -0.4942  &        0.030     &   -16.702  &         0.000        &       -0.552    &       -0.436     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Generalized Linear Model Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                  fraud   No. Observations:                78300\n",
       "Model:                            GLM   Df Residuals:                    78294\n",
       "Model Family:                Binomial   Df Model:                            5\n",
       "Link Function:                  Logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -20496.\n",
       "Date:                Tue, 29 Apr 2025   Deviance:                       40991.\n",
       "Time:                        19:40:28   Pearson chi2:                 9.30e+04\n",
       "No. Iterations:                     9   Pseudo R-squ. (CS):            0.05985\n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const              -3.1605      0.033    -97.073      0.000      -3.224      -3.097\n",
       "used_pin_number    -3.6261      0.209    -17.315      0.000      -4.037      -3.216\n",
       "log_DFH_padro       0.3313      0.015     22.826      0.000       0.303       0.360\n",
       "log_DFLT_padro      0.2085      0.013     15.814      0.000       0.183       0.234\n",
       "RROO                1.3823      0.035     39.484      0.000       1.314       1.451\n",
       "used_chip          -0.4942      0.030    -16.702      0.000      -0.552      -0.436\n",
       "===================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd53848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Fraude_CC/vFraude_CC/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    with mlflow.start_run(run_name = 'LR_Sample_VarOrigCria', experiment_id = experiment.experiment_id) as run: \n",
    "        \n",
    "        signature = infer_signature(X_treino, LR_results.predict(sm.add_constant(X_treino)))\n",
    "        mlflow.statsmodels.log_model(LR_results, signature=signature, artifact_path='modelo')\n",
    "        \n",
    "        mlflow.log_params(LR_results.params)\n",
    "\n",
    "        # Log das métricas na base de TREINO\n",
    "        mlflow.log_metric('AUC_PR_Treino', average_precision_score(y_treino, LR_results.predict(sm.add_constant(X_treino))))\n",
    "        mlflow.log_metric('AUC_ROC_Treino', roc_auc_score(y_treino, LR_results.predict(sm.add_constant(X_treino))))\n",
    "        mlflow.log_metric('BS_Treino', brier_score_loss(y_treino, LR_results.predict(sm.add_constant(X_treino))))\n",
    "        mlflow.log_metric('Log_Loss_Treino', log_loss(y_treino, LR_results.predict(sm.add_constant(X_treino)), normalize=True))\n",
    "\n",
    "        # Log das métricas na base de VALIDAÇÃO\n",
    "        mlflow.log_metric('AUC_PR_Val', average_precision_score(y_val, LR_results.predict(sm.add_constant(X_val))))\n",
    "        mlflow.log_metric('AUC_ROC_Val', roc_auc_score(y_val, LR_results.predict(sm.add_constant(X_val))))\n",
    "        mlflow.log_metric('BS_Val', brier_score_loss(y_val, LR_results.predict(sm.add_constant(X_val))))\n",
    "        mlflow.log_metric('Log_Loss_Val', log_loss(y_val, LR_results.predict(sm.add_constant(X_val)), normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac07021",
   "metadata": {},
   "source": [
    "## Função objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9770c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_objetivo_CV(parametros, modelo, folds, expr, X, y):\n",
    "    # função objetivo para \"minimizar\", mas dependendo da métrica de interesse, na realidade, é maximizar \n",
    "    # parametros é o espaço paramétrico a ser explorado\n",
    "    # expr é uma string que representa o id do experimento que foi criado\n",
    "    # modelo é uma string de qual modelo será rodado: Random Forest ou XGBoost\n",
    "    # folds é um int que diz quantos folds de validação serão usados\n",
    "    # X e y são as bases que serão aplicadas o cross-validation\n",
    "\n",
    "    # O output é o valor do score a ser minimizado/maximizado\n",
    "    \n",
    "    with mlflow.start_run(nested = True, experiment_id=expr) as run:\n",
    "\n",
    "        SKF = StratifiedKFold(n_splits = folds, shuffle=True, random_state=1234)\n",
    "\n",
    "        if modelo == 'RF':\n",
    "            clf = RandomForestClassifier(**parametros) \n",
    "            clf.fit(X, y)\n",
    "        elif modelo == 'XGB':\n",
    "            clf = xgboost.XGBClassifier(**parametros)\n",
    "            clf.fit(X, y)\n",
    "        elif modelo == 'CAT':\n",
    "            clf = CatBoostClassifier(**parametros, verbose=False, cat_features = ['used_chip', 'repeat_retailer', 'used_pin_number', 'online_order'], loss_function = 'Logloss')\n",
    "            clf.fit(X, y)\n",
    "        elif modelo == 'LGBM':\n",
    "            clf = lightgbm.LGBMClassifier(**parametros, verbose=-1)\n",
    "            clf.fit(X, y)\n",
    "        \n",
    "        score = cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='neg_log_loss').mean()\n",
    "\n",
    "        # Log de parâmetros e métricas\n",
    "\n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric('average_precision_cv', cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='average_precision').mean())\n",
    "        mlflow.log_metric('roc_auc_cv', cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='roc_auc').mean())\n",
    "        mlflow.log_metric('neg_brier_score_cv', cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='neg_brier_score').mean())\n",
    "        mlflow.log_metric('neg_log_loss_cv', cross_val_score(estimator = clf, X = X, y = y, cv = SKF, scoring='neg_log_loss').mean())\n",
    "        \n",
    "        signature = infer_signature(X, clf.predict_proba(X))\n",
    "        mlflow.sklearn.log_model(clf, signature=signature, artifact_path='modelo')\n",
    "\n",
    "    return -score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc372308",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b99c1191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Fraude_CC/vFraude_CC/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [01:13<29:32, 73.86s/trial, best loss: 0.22249764798969957]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Fraude_CC/vFraude_CC/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Com tunning de hiperparâmetros\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #test_imbalanced = [{0: len(y_treino2)/(2*np.bincount(y_treino2))[0], 1:len(y_teste2)/(2*np.bincount(y_teste2))[1]}, {0: 1, 1:1}]\n",
    "    \n",
    "    space = {\n",
    "        \"n_estimators\": hp.choice('n_estimators', np.arange(10, 100, dtype=int)),\n",
    "        \"max_depth\": hp.choice('max_depth', np.arange(10, 50, dtype=int)),\n",
    "        \"min_samples_leaf\": hp.choice('min_samples_leaf', np.arange(100, 500, dtype=int)),\n",
    "        \"min_samples_split\": hp.choice('min_samples_split', np.arange(100, 500, dtype=int)),\n",
    "        \"criterion\": hp.choice(\"criterion\", ['gini', 'entropy', 'log_loss']),\n",
    "        \"class_weight\": hp.choice(\"class_weight\", ['balanced', 'balanced_subsample', None]),\n",
    "        \"max_features\": hp.uniform(\"max_features\", 0.1, 1),\n",
    "        \"max_samples\": hp.uniform(\"max_samples\", 0.1, 1)\n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(run_name = 'RF_Sample_VarOrigCria_CV', experiment_id=experiment.experiment_id) as run:\n",
    "        best_params = fmin(\n",
    "            fn = partial(\n",
    "                func_objetivo_CV,\n",
    "                expr = experiment.experiment_id,\n",
    "                modelo = 'RF',\n",
    "                X = X_treino,\n",
    "                y = y_treino,\n",
    "                folds = 5\n",
    "            ),\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 25,\n",
    "            trials = Trials(),\n",
    "            timeout = 10\n",
    "        )\n",
    "\n",
    "        if best_params['criterion'] == 0:\n",
    "            best_params['criterion'] = 'gini'\n",
    "        elif best_params['criterion'] == 1:\n",
    "            best_params['criterion'] = 'entropy'\n",
    "        else:\n",
    "            best_params['criterion'] = 'log_loss'\n",
    "            \n",
    "\n",
    "        if best_params['class_weight'] == 0:\n",
    "            best_params['class_weight'] = 'balanced'\n",
    "        elif best_params['class_weight'] == 1:\n",
    "            best_params['class_weight'] = 'balanced_subsample'\n",
    "        else:\n",
    "            best_params['class_weight'] = None\n",
    "        \n",
    "        # Identificado o melhor conjunto de hiperparâmetros, treina o modelo com toda a base de treino e metrifica os escores na base de validação\n",
    "\n",
    "        clf = RandomForestClassifier(**best_params)\n",
    "        clf.fit(X_treino, y_treino)\n",
    "                   \n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric('AUC_PR_Val', average_precision_score(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "        mlflow.log_metric('Log_Loss_Val', log_loss(y_val, clf.predict_proba(X_val)[:,1], normalize=True))\n",
    "        mlflow.log_metric('AUC_ROC_Val', roc_auc_score(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "        mlflow.log_metric('BS_Val', brier_score_loss(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "\n",
    "        # Log das métricas na base de TREINO\n",
    "        mlflow.log_metric('AUC_PR_Treino', average_precision_score(y_treino, clf.predict_proba(X_treino)[:,1]))\n",
    "        mlflow.log_metric('AUC_ROC_Treino', roc_auc_score(y_treino, clf.predict_proba(X_treino)[:,1]))\n",
    "        mlflow.log_metric('BS_Treino', brier_score_loss(y_treino, clf.predict_proba(X_treino)[:,1]))\n",
    "        mlflow.log_metric('Log_Loss_Treino', log_loss(y_treino, clf.predict_proba(X_treino)[:,1], normalize=True))\n",
    "\n",
    "        signature = infer_signature(X_treino, clf.predict_proba(X_treino))\n",
    "        mlflow.sklearn.log_model(clf, signature=signature, artifact_path='modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5526aa2",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f948bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Fraude_CC/vFraude_CC/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:13<05:16, 13.20s/trial, best loss: 0.24764082426350761]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Fraude_CC/vFraude_CC/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Com tunning de hiperparâmetros\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    space = {\n",
    "        \"n_estimators\": hp.choice('n_estimators', np.arange(10, 150, dtype=int)),\n",
    "        \"max_depth\": hp.choice('max_depth', np.arange(10, 150, dtype=int)),\n",
    "        \"min_child_weight\": hp.choice('min_child_weight', np.arange(100, 500, dtype=int)),\n",
    "        \"colsample_bytree\": hp.quniform('colsample_bytree', 0.2, 1, 0.05),\n",
    "        \"colsample_bylevel\": hp.quniform('colsample_bylevel', 0.2, 1, 0.05),\n",
    "        \"subsample\": hp.quniform('subsample', 0.2, 1, 0.05),\n",
    "        \"colsample_bynode\": hp.quniform('colsample_bynode', 0.2, 1, 0.05),\n",
    "        \"base_score\": hp.quniform('base_score', 0.1, 1, 0.05),\n",
    "        \"learning_rate\": hp.quniform('learning_rate', 0.0025, 0.5, 0.025),\n",
    "        \"gamma\": hp.choice('gamma', np.arange(1, 20, dtype=int)),\n",
    "        \"lambda\": hp.choice('lambda', np.arange(1, 20, dtype=int)),\n",
    "        \"alpha\": hp.choice('alpha', np.arange(1, 20, dtype=int)),\n",
    "\n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(run_name = 'XGB_Sample_VarOrigCria_CV', experiment_id=experiment.experiment_id) as run:\n",
    "        best_params = fmin(\n",
    "            fn = partial(\n",
    "                func_objetivo_CV,\n",
    "                expr = experiment.experiment_id,\n",
    "                modelo = 'XGB',\n",
    "                X = X_treino,\n",
    "                y = y_treino,\n",
    "                folds = 5\n",
    "            ),\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 25,\n",
    "            trials = Trials(),\n",
    "            timeout = 10\n",
    "        )\n",
    "        \n",
    "        # Identificado o melhor conjunto de hiperparâmetros, treina o modelo com toda a base de treino e metrifica os escores na base de validação\n",
    "\n",
    "        clf = xgboost.XGBClassifier(**best_params)\n",
    "        clf.fit(X_treino, y_treino)\n",
    "                   \n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric('AUC_PR_Val', average_precision_score(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "        mlflow.log_metric('Log_Loss_Val', log_loss(y_val, clf.predict_proba(X_val)[:,1], normalize=True))\n",
    "        mlflow.log_metric('AUC_ROC_Val', roc_auc_score(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "        mlflow.log_metric('BS_Val', brier_score_loss(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "\n",
    "        # Log das métricas na base de TREINO\n",
    "        mlflow.log_metric('AUC_PR_Treino', average_precision_score(y_treino, clf.predict_proba(X_treino)[:,1]))\n",
    "        mlflow.log_metric('AUC_ROC_Treino', roc_auc_score(y_treino, clf.predict_proba(X_treino)[:,1]))\n",
    "        mlflow.log_metric('BS_Treino', brier_score_loss(y_treino, clf.predict_proba(X_treino)[:,1]))\n",
    "        mlflow.log_metric('Log_Loss_Treino', log_loss(y_treino, clf.predict_proba(X_treino)[:,1], normalize=True))\n",
    "\n",
    "        signature = infer_signature(X_treino, clf.predict_proba(X_treino))\n",
    "        mlflow.sklearn.log_model(clf, signature=signature, artifact_path='modelo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e912a8c",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dd47bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Fraude_CC/vFraude_CC/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:06<02:33,  6.39s/trial, best loss: 0.24362677219584725]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Fraude_CC/vFraude_CC/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:14<02:45,  7.20s/trial, best loss: 0.2130010421570317] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/Git_GitHub/Estudo_Fraude_CC/vFraude_CC/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Com tunning de hiperparâmetros\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    space = {\n",
    "        \"n_estimators\": hp.choice('n_estimators', np.arange(10, 150, dtype=int)), \n",
    "        \"max_depth\": hp.choice('max_depth', np.arange(10, 50, dtype=int)), \n",
    "        \"min_child_weight\": hp.choice('min_child_weight', np.arange(100, 500, dtype=int)),\n",
    "        \"colsample_bytree\": hp.quniform('colsample_bytree', 0.2, 1, 0.05),\n",
    "        \"subsample\": hp.quniform('subsample', 0.2, 1, 0.05), \n",
    "        \"colsample_bytree\": hp.quniform('colsample_bynode', 0.2, 1, 0.05),\n",
    "        \"learning_rate\": hp.quniform('learning_rate', 0.0025, 0.5, 0.025), \n",
    "        \"reg_lambda\": hp.choice('reg_lambda', np.arange(1, 20, dtype=float)),\n",
    "        \"reg_alpha\": hp.choice('reg_alpha', np.arange(1, 20, dtype=float))\n",
    "\n",
    "    }\n",
    "    \n",
    "    with mlflow.start_run(run_name = 'LGBM_Sample_VarOrigCria_CV', experiment_id=experiment.experiment_id) as run:\n",
    "        best_params = fmin(\n",
    "            fn = partial(\n",
    "                func_objetivo_CV,\n",
    "                expr = experiment.experiment_id,\n",
    "                modelo = 'LGBM',\n",
    "                X = X_treino,\n",
    "                y = y_treino,\n",
    "                folds = 5\n",
    "            ),\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 25,\n",
    "            trials = Trials(),\n",
    "            timeout = 10\n",
    "        )\n",
    "        \n",
    "        # Identificado o melhor conjunto de hiperparâmetros, treina o modelo com toda a base de treino e metrifica os escores na base de validação\n",
    "\n",
    "        clf = lightgbm.LGBMClassifier(**best_params)\n",
    "        clf.fit(X_treino, y_treino)\n",
    "                   \n",
    "        mlflow.log_params(clf.get_params())\n",
    "        mlflow.log_metric('AUC_PR_Val', average_precision_score(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "        mlflow.log_metric('Log_Loss_Val', log_loss(y_val, clf.predict_proba(X_val)[:,1], normalize=True))\n",
    "        mlflow.log_metric('AUC_ROC_Val', roc_auc_score(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "        mlflow.log_metric('BS_Val', brier_score_loss(y_val, clf.predict_proba(X_val)[:,1]))\n",
    "\n",
    "        # Log das métricas na base de TREINO\n",
    "        mlflow.log_metric('AUC_PR_Treino', average_precision_score(y_treino, clf.predict_proba(X_treino)[:,1]))\n",
    "        mlflow.log_metric('AUC_ROC_Treino', roc_auc_score(y_treino, clf.predict_proba(X_treino)[:,1]))\n",
    "        mlflow.log_metric('BS_Treino', brier_score_loss(y_treino, clf.predict_proba(X_treino)[:,1]))\n",
    "        mlflow.log_metric('Log_Loss_Treino', log_loss(y_treino, clf.predict_proba(X_treino)[:,1], normalize=True))\n",
    "\n",
    "        signature = infer_signature(X_treino, clf.predict_proba(X_treino))\n",
    "        mlflow.sklearn.log_model(clf, signature=signature, artifact_path='modelo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vFraude_CC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
